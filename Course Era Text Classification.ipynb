{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying Auto Correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autocorrect import spell\n",
    "print(spell('caaaar'))\n",
    "print(spell(u'mussage'))\n",
    "print (spell(u'survice'))\n",
    "print (spell(u'hte'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n",
      "C:\\Users\\M1030574\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\M1030574\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\M1030574\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\M1030574\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\M1030574\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\M1030574\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\M1030574\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\M1030574\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\M1030574\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\M1030574\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\M1030574\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\M1030574\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "##Loading required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='C:\\\\Users\\\\M1030574\\\\Desktop\\\\NLP COE\\\\Text Classification\\\\Course Era Classification\\\\'\n",
    "filename='reviews.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(path+filename)\n",
    "##Groupin 5 labels to 3 labels as positive neutral and negative\n",
    "data['Label']=data['Label'].replace({1:1,2:1,3:2,4:3,5:3})\n",
    "label1=data[data['Label']==1].sample(n=700,random_state=1234)\n",
    "label2=data[data['Label']==2].sample(n=500,random_state=1234)\n",
    "label3=data[data['Label']==3].sample(n=1300,random_state=1234)\n",
    "data1=pd.concat([label1,label2,label3])\n",
    "##Renaming label codes to 0,1,2 for label names \n",
    "data1['Label']=data['Label'].replace({1:0,2:1,3:2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1,2>negative-1\n",
    "# 3>neutral-2\n",
    "# 4,5>positive-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(data1['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Summarizing data\n",
    "data1.groupby(\"Label\").agg({'Id':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['Review']=data1['Review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##Converting it into codes \n",
    "# data1['Label']= pd.Categorical(data1['Label'])\n",
    "# data1['Label_Code']=data1['Label'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train,y_test = train_test_split(data1[['Id','Review']],data1[['Label']], stratify=data1['Label'],\n",
    "                                                   test_size=0.3, random_state=42)\n",
    "Train=pd.concat([x_train['Review'],y_train['Label']],axis=1)\n",
    "Test=pd.concat([x_test['Review'],y_test['Label']],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.to_csv(path+\"Processed_Data.csv\")\n",
    "Train.to_csv(path+\"Train_Data.csv\")\n",
    "Test.to_csv(path+\"Test_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train=pd.read_csv(path+\"Train_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train=Train[['Review','Label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional model configuration\n",
    "model_args = ClassificationArgs(num_train_epochs=5,overwrite_output_dir=True)\n",
    "\n",
    "print(\"model_args\")\n",
    "\n",
    "\n",
    "# bert:Bert,bert-base-uncased\n",
    "\n",
    "# xlnet:xlnet,xlnet-base-cased  \n",
    "#'roberta','roberta-base',\n",
    "#electra-base,google/electra-base-discriminator\"\n",
    "#\"google/electra-small-discriminator,electra\n",
    "\n",
    "# Create a ClassificationModel\n",
    "model_bert = ClassificationModel(\n",
    "    model_type='roberta',\n",
    "    model_name=\"roberta-base\",\n",
    "    num_labels=3,\n",
    "    args=model_args,\n",
    "#     use_cuda=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"model_traing\")\n",
    "# Train the model\n",
    "model_bert.train_model(Train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_Name='electra_small_3.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_bert, open(path+Model_Name,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading Model and checking accuracy\n",
    "Model_Name='electra_small_3'\n",
    "model=pickle.load(open(path+Model_Name+'.sav','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f8da02bee995>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m##Accuracy for train data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mTrain_Predictions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Review'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mTrain_Prediction_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrain_Predictions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTrain_Prediction_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Train' is not defined"
     ]
    }
   ],
   "source": [
    "##Accuracy for train data \n",
    "Train_Predictions=model.predict(Train['Review'].tolist())\n",
    "Train_Prediction_df=pd.DataFrame(Train_Predictions[0])\n",
    "print(accuracy_score(Train[['Label']],Train_Prediction_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Final DataFrame for traindata to check accuracy\n",
    "Train_Op=pd.concat([Train[['Review','Label']].reset_index(drop=True),Train_Prediction_df.reset_index(drop=True)],axis=1)\n",
    "Train_Op.columns=['Review','Actual_Label','Predicted_Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Op.to_csv(path+'Train_Op'+Model_Name+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test=pd.read_csv(path+\"Test_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test=Test[['Review','Label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Accuracy for test data \n",
    "Test_Predictions=model.predict(Test['Review'].tolist())\n",
    "Test_Prediction_df=pd.DataFrame(Test_Predictions[0])\n",
    "print(accuracy_score(Test[['Label']],Test_Prediction_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Final DataFrame for testdata to check accuracy\n",
    "Test_Op=pd.concat([Test.reset_index(drop=True),Test_Prediction_df.reset_index(drop=True)],axis=1)\n",
    "Test_Op.columns=['Review','Actual_Label','Predicted_Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Op.to_csv(path+'Test_Op'+Model_Name+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##lABEL WISE CHECK FOR IT\n",
    "##Reading Train Output\n",
    "Train_op=pd.read_csv(path+'Train_Op'+Model_Name+'.csv')\n",
    "Train_op=Train_op[['Review','Actual_Label','Predicted_Label']]\n",
    "Train_op_Label0=Train_op[Train_op['Actual_Label']==0]\n",
    "Train_op_Label1=Train_op[Train_op['Actual_Label']==1]\n",
    "Train_op_Label2=Train_op[Train_op['Actual_Label']==2]\n",
    "print('TrainLabel_0='+str(accuracy_score(Train_op_Label0['Actual_Label'],Train_op_Label0['Predicted_Label'])))\n",
    "print('TrainLabel_1='+str(accuracy_score(Train_op_Label1['Actual_Label'],Train_op_Label1['Predicted_Label'])))\n",
    "print('TrainLabel_2='+str(accuracy_score(Train_op_Label2['Actual_Label'],Train_op_Label2['Predicted_Label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##lABEL WISE CHECK FOR IT\n",
    "##Reading Test Output\n",
    "Test_op=pd.read_csv(path+'Test_Op'+Model_Name+'.csv')\n",
    "Test_op=Test_op[['Review','Actual_Label','Predicted_Label']]\n",
    "Test_op_Label0=Test_op[Test_op['Actual_Label']==0]\n",
    "Test_op_Label1=Test_op[Test_op['Actual_Label']==1]\n",
    "Test_op_Label2=Test_op[Test_op['Actual_Label']==2]\n",
    "print('TestLabel_0='+str(accuracy_score(Test_op_Label0['Actual_Label'],Test_op_Label0['Predicted_Label'])))\n",
    "print('TestLabel_1='+str(accuracy_score(Test_op_Label1['Actual_Label'],Test_op_Label1['Predicted_Label'])))\n",
    "print('TestLabel_2='+str(accuracy_score(Test_op_Label2['Actual_Label'],Test_op_Label2['Predicted_Label'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Model and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loading Model and checking accuracy\n",
    "path='C:\\\\Users\\\\M1030574\\\\Desktop\\\\NLP COE\\\\Text Classification\\\\Course Era Classification\\\\'\n",
    "Model_Name='electra_small_3'\n",
    "model=pickle.load(open(path+Model_Name+'.sav','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66f21dd1dba4a33bed2f3dc313d7e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f134e982d60445781ca3dabed217edc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text=[\"i ve already written a review for part 1 and i have the same opinion about this one. the course is rather poor and not challenging. only general information about relevant topics that as well read on wikipedia. no exercises, no code assignments. a lot of this content was repeated from first two parts of this specialization.\"]\n",
    "predicted=model.predict(text)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = {0 : \"Negative\", 1 : \"Neutral\",2:\"Positive\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i ve already written a review for part 1 and i have the same opinion about this one. the course is rather poor and not challenging. only general information about relevant topics that as well read on wikipedia. no exercises, no code assignments. a lot of this content was repeated from first two parts of this specialization.'] Negative\n"
     ]
    }
   ],
   "source": [
    "for key,value in label.items():\n",
    "    if key == predicted:\n",
    "         print(text,value)\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation for negative feedbacks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Bert Model\n",
    "import pandas as pd\n",
    "##Reasing Train and Test Bert Output \n",
    "Train_op_bert=pd.read_csv(\"C:\\\\Users\\\\M1030574\\\\Desktop\\\\NLP COE\\\\Text Classification\\\\Course Era Classification\\\\Train_Opbert.csv\")\n",
    "Test_op_bert=pd.read_csv(\"C:\\\\Users\\\\M1030574\\\\Desktop\\\\NLP COE\\\\Text Classification\\\\Course Era Classification\\\\Test_Opbert.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_op_bert.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_op_bert_negative=Train_op_bert[Train_op_bert['Actual_Label']==0]\n",
    "Test_op_bert_negative=Test_op_bert[Test_op_bert['Actual_Label']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_op_bert_negative.groupby(\"Predicted_Label\").agg({'Review':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_op_bert_negative.groupby(\"Predicted_Label\").agg({'Review':'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Bert Model\n",
    "import pandas as pd\n",
    "##Reasing Train and Test Bert Output \n",
    "Train_op_Roberta=pd.read_csv(\"C:\\\\Users\\\\M1030574\\\\Desktop\\\\NLP COE\\\\Text Classification\\\\Course Era Classification\\\\Train_Oproberta.csv\")\n",
    "Test_op_Roberta=pd.read_csv(\"C:\\\\Users\\\\M1030574\\\\Desktop\\\\NLP COE\\\\Text Classification\\\\Course Era Classification\\\\Test_Oproberta.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_op_Roberta_negative=Train_op_Roberta[Train_op_Roberta['Actual_Label']==0]\n",
    "Test_op_Roberta_negative=Test_op_Roberta[Test_op_Roberta['Actual_Label']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_op_Roberta_negative.groupby(\"Predicted_Label\").agg({'Review':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_op_Roberta_negative.groupby(\"Predicted_Label\").agg({'Review':'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XLNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Bert Model\n",
    "import pandas as pd\n",
    "##Reasing Train and Test Bert Output \n",
    "Train_op_xlnet=pd.read_csv(\"C:\\\\Users\\\\M1030574\\\\Desktop\\\\NLP COE\\\\Text Classification\\\\Course Era Classification\\\\Train_Opxlnet.csv\")\n",
    "Test_op_xlnet=pd.read_csv(\"C:\\\\Users\\\\M1030574\\\\Desktop\\\\NLP COE\\\\Text Classification\\\\Course Era Classification\\\\Test_Opxlnet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_op_xlnet_negative=Train_op_xlnet[Train_op_xlnet['Actual_Label']==0]\n",
    "Test_op_xlnet_negative=Test_op_xlnet[Test_op_xlnet['Actual_Label']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_op_xlnet_negative.groupby(\"Predicted_Label\").agg({'Review':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_op_xlnet_negative.groupby(\"Predicted_Label\").agg({'Review':'count'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating model Outouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Op_Bert=pd.read_csv(\"C:\\\\Users\\\\M1030574\\\\Desktop\\\\NLP COE\\\\Text Classification\\\\Course Era Classification\\\\Train_Opbert.csv\")\n",
    "Train_Op_Roberta=pd.read_csv(\"C:\\\\Users\\\\M1030574\\\\Desktop\\\\NLP COE\\\\Text Classification\\\\Course Era Classification\\\\Train_Oproberta.csv\")\n",
    "Train_Op_Xlnet=pd.read_csv(\"C:\\\\Users\\\\M1030574\\\\Desktop\\\\NLP COE\\\\Text Classification\\\\Course Era Classification\\\\Train_Opxlnet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Op_Bert=Train_Op_Bert[['Review','Actual_Label','Predicted_Label']]\n",
    "Train_Op_Bert.columns=['Review','Actual_Label','Predicted_Label_Bert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Op_Roberta=Train_Op_Roberta[['Review','Predicted_Label']]\n",
    "Train_Op_Roberta.columns=['Review_Roberta','Predicted_Label_Roberta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Op_Xlnet=Train_Op_Xlnet[['Review','Predicted_Label']]\n",
    "Train_Op_Xlnet.columns=['Review_Xlnet','Predicted_Label_Xlnet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Concatenated_Op=pd.concat([Train_Op_Bert,Train_Op_Roberta,Train_Op_Xlnet],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Concatenated_Op.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Concatenated_Op=Train_Concatenated_Op[['Review','Actual_Label','Predicted_Label_Bert','Predicted_Label_Roberta','Predicted_Label_Xlnet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Concatenated_Op.to_excel('C:\\\\Users\\\\M1030574\\\\Desktop\\\\NLP COE\\\\Text Classification\\\\Course Era Classification\\\\Train_Op_Count.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Doing Same for Test data \n",
    "Test_Op_Bert=pd.read_csv(\"C:\\\\Users\\\\M1030574\\\\Desktop\\\\NLP COE\\\\Text Classification\\\\Course Era Classification\\\\Test_Opbert.csv\")\n",
    "Test_Op_Roberta=pd.read_csv(\"C:\\\\Users\\\\M1030574\\\\Desktop\\\\NLP COE\\\\Text Classification\\\\Course Era Classification\\\\Test_Oproberta.csv\")\n",
    "Test_Op_Xlnet=pd.read_csv(\"C:\\\\Users\\\\M1030574\\\\Desktop\\\\NLP COE\\\\Text Classification\\\\Course Era Classification\\\\Test_Opxlnet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Op_Bert=Test_Op_Bert[['Review','Actual_Label','Predicted_Label']]\n",
    "Test_Op_Bert.columns=['Review','Actual_Label','Predicted_Label_Bert']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Op_Roberta=Test_Op_Roberta[['Review','Predicted_Label']]\n",
    "Test_Op_Roberta.columns=['Review_Roberta','Predicted_Label_Roberta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Op_Xlnet=Test_Op_Xlnet[['Review','Predicted_Label']]\n",
    "Test_Op_Xlnet.columns=['Review_Xlnet','Predicted_Label_Xlnet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Concatenated_Op=pd.concat([Test_Op_Bert,Test_Op_Roberta,Test_Op_Xlnet],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Concatenated_Op=Test_Concatenated_Op[['Review','Actual_Label','Predicted_Label_Bert','Predicted_Label_Roberta','Predicted_Label_Xlnet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Concatenated_Op.to_excel('C:\\\\Users\\\\M1030574\\\\Desktop\\\\NLP COE\\\\Text Classification\\\\Course Era Classification\\\\Test_Op_Count.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Concatenated_Op['Probality_label0']=pd.concat([Test_Concatenated_Op.Predicted_Label_Bert==0,Test_Concatenated_Op.Predicted_Label_Roberta==0,Test_Concatenated_Op.Predicted_Label_Xlnet==0],axis=1).sum(axis=1)\n",
    "Test_Concatenated_Op['Probality_label1']=pd.concat([Test_Concatenated_Op.Predicted_Label_Bert==1,Test_Concatenated_Op.Predicted_Label_Roberta==1,Test_Concatenated_Op.Predicted_Label_Xlnet==1],axis=1).sum(axis=1)\n",
    "Test_Concatenated_Op['Probality_label2']=pd.concat([Test_Concatenated_Op.Predicted_Label_Bert==2,Test_Concatenated_Op.Predicted_Label_Roberta==2,Test_Concatenated_Op.Predicted_Label_Xlnet==2],axis=1).sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Concatenated_Op['Probality_label0']=Test_Concatenated_Op['Probality_label0']/3\n",
    "Test_Concatenated_Op['Probality_label1']=Test_Concatenated_Op['Probality_label1']/3\n",
    "Test_Concatenated_Op['Probality_label2']=Test_Concatenated_Op['Probality_label2']/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Concatenated_Op['Max_Probality']=Test_Concatenated_Op[[\"Probality_label0\", \"Probality_label1\",\"Probality_label2\"]].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Concatenated_Op['Predicted_Label']=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Concatenated_Op['Predicted_Label'].loc[Test_Concatenated_Op['Max_Probality']==Test_Concatenated_Op['Probality_label0']] = 0\n",
    "Test_Concatenated_Op['Predicted_Label'].loc[Test_Concatenated_Op['Max_Probality']==Test_Concatenated_Op['Probality_label1']] = 1\n",
    "Test_Concatenated_Op['Predicted_Label'].loc[Test_Concatenated_Op['Max_Probality']==Test_Concatenated_Op['Probality_label2']] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(Test_Concatenated_Op['Predicted_Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Concatenated_Op['Accuracy']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_Concatenated_Op['Accuracy'].loc[Test_Concatenated_Op['Predicted_Label']==Test_Concatenated_Op['Actual_Label']]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(Test_Concatenated_Op['Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(Test_Concatenated_Op['Accuracy'])/np.count(Test_Concatenated_Op['Accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
